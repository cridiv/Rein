# ML Infrastructure Configuration

## Environment Variables

### Opik Configuration
# Opik API Key (obtain from https://app.opik.ai)
OPIK_API_KEY=your_opik_api_key_here

# Opik project name (where traces will be stored)
OPIK_PROJECT_NAME=rein-ai-coaching

# Opik workspace URL (optional, defaults to production)
OPIK_WORKSPACE_URL=https://api.opik.ai

### Google Gemini Configuration
# Google AI API Key for Gemini 2.0 Flash
GEMINI_API_KEY=your_gemini_api_key_here

### Database Configuration
# PostgreSQL connection for storing traces and feedback
DATABASE_URL=postgresql://user:password@localhost:5432/rein_ml

### Redis Configuration (for caching and session management)
# Redis connection string for caching LLM responses
REDIS_URL=redis://localhost:6379

### Model Configuration
# Temperature for LLM generation (0.0-1.0)
LLM_TEMPERATURE=0.7

# Maximum output tokens for LLM
LLM_MAX_TOKENS=2000

# Batch processing configuration
BATCH_SIZE=10
BATCH_TIMEOUT_MS=5000

### Evaluation Configuration
# Threshold for resolution quality score (0-10)
QUALITY_SCORE_THRESHOLD=6.5

# Minimum feedback count before generating insights
MIN_FEEDBACK_FOR_INSIGHTS=5

### Application Configuration
# Environment (development, staging, production)
NODE_ENV=development

# Port for backend server
PORT=3000

# Backend URL for frontend consumption
BACKEND_URL=http://localhost:3000

### Logging Configuration
# Log level (error, warn, info, debug)
LOG_LEVEL=info

# Enable detailed trace logging
TRACE_LOGGING_ENABLED=true
